{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "codebertsimilar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNG54FVaBnPzkRUDTHpoEZp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamW1002/597Fill/blob/main/codebertsimilar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0VZBFbTlCR-",
        "outputId": "56b1de50-0bde-453f-ddbc-7ee867d93c8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 4.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 39.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 46.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 46.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tylcpscmoXQJ",
        "outputId": "5bac7b6c-463b-49a4-c4fb-bfe5ec38531c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "   creating: content/CodeT5/data/clone/\n",
            "  inflating: content/CodeT5/data/clone/train.txt  \n",
            "  inflating: content/CodeT5/data/clone/data.jsonl  \n",
            "  inflating: content/CodeT5/data/clone/test.txt  \n",
            "  inflating: content/CodeT5/data/clone/valid.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
        "MAX_TOKEN_DIM = 384 #controls padding and input to classifier"
      ],
      "metadata": {
        "id": "U-Ip-3YLpFJn"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "  f = open(\"data.jsonl\") #read sniipets and indices\n",
        "  entries = f.readlines()\n",
        "  objects = [json.loads(x) for x in entries] #load all functions\n",
        "  idx_to_function = dict()\n",
        " \n",
        "  for snippet in objects:#map to associate index to func\n",
        "    \n",
        "    idx_to_function[snippet[\"idx\"]] = snippet[\"func\"]\n",
        "\n",
        "  return idx_to_function"
      ],
      "metadata": {
        "id": "xhrTHlM6pOfa"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pairify_file(lines : list, idx_to_function : dict) -> tuple:\n",
        "  examples = []\n",
        "  \n",
        "  for line in lines:\n",
        "    line_entries = line.replace(\"\\t\", \" \").split(\" \") #given line x y label, divide to find if x is y according to label\n",
        "    #print(line)\n",
        "    x = line_entries[0]\n",
        "    y = line_entries[1]\n",
        "    label = line_entries[2]\n",
        "    \n",
        "    examples.append((idx_to_function[x], idx_to_function[y], float(label))) #convert label to float for pytorch\n",
        "  return examples\n"
      ],
      "metadata": {
        "id": "UiomZTWIp0rQ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_and_label_data(idx_to_function : dict): #convert pairs to useful training examples\n",
        "  return tuple(map(  lambda x : pairify_file(open(x).readlines(), idx_to_function)  , [\"train.txt\",\"test.txt\", \"valid.txt\"]))\n"
      ],
      "metadata": {
        "id": "49GUPqpspx2P"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_to_function = load_data()\n",
        "train_data, test_data,validation_data = split_and_label_data(idx_to_function)"
      ],
      "metadata": {
        "id": "hafzrTWspZ1P"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YQtpDvNstzDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_data[0][0])\n",
        "print(test_data[0][1])\n",
        "print(test_data[0][2])"
      ],
      "metadata": {
        "id": "WzSahqZgtgr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed(x : str) -> tuple:\n",
        "  code_tokens=tokenizer.tokenize(x)\n",
        "  tokens=[tokenizer.cls_token]+code_tokens[:510]+[tokenizer.sep_token]\n",
        "\n",
        "  tokens_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
        "  context_embeddings=model(torch.tensor(tokens_ids)[None,:])[0]\n",
        "  return torch.flatten(context_embeddings) #return flattened embedding vector"
      ],
      "metadata": {
        "id": "QLJLCUu4w0PY"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        # Number of input features is 12.\n",
        "        #self.layer_1 = nn.Linear(12, 64) \n",
        "        #self.layer_2 = nn.Linear(64, 64)\n",
        "        #self.layer_out = nn.Linear(64, 1) \n",
        "        #\n",
        "        #self.relu = nn.ReLU()\n",
        "        #self.dropout = nn.Dropout(p=0.1)\n",
        "        #self.batchnorm1 = nn.BatchNorm1d(64)\n",
        "        #self.batchnorm2 = nn.BatchNorm1d(64)\n",
        "        \n",
        "        #A note on architecture for those interested, we eat CodeBERT embeddings of size X  * 768 which have been flattened\n",
        "        # Now those vectors are each fed into FF layer(s)\n",
        "        #Then they're concatnated and fed thru more FF layer(s)\n",
        "        # Then their dimensionality is shrunk down to 1, which is sigmoided\n",
        "        layer2_size = 256\n",
        "        self.xlayer_1 = nn.Linear(MAX_TOKEN_DIM * 768, layer2_size)\n",
        "        self.ylayer_1 = nn.Linear(MAX_TOKEN_DIM * 768, layer2_size)\n",
        "\n",
        "        self.ff1 = nn.Linear(2 * layer2_size, 1 )\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x,y):\n",
        "       #x = self.relu(self.layer_1(inputs))\n",
        "       #x = self.batchnorm1(x)\n",
        "       #x = self.relu(self.layer_2(x))\n",
        "       #x = self.batchnorm2(x)\n",
        "       #x = self.dropout(x)\n",
        "       #x = self.layer_out(x)\n",
        "       #\n",
        "       #return x\n",
        "       xtemp = self.xlayer_1(x)\n",
        "       xtemp = self.relu(xtemp)\n",
        "\n",
        "       ytemp = self.ylayer_1(y)\n",
        "       ytemp = self.relu(ytemp)\n",
        "\n",
        "       combined = torch.cat((xtemp, ytemp),0)\n",
        "       out = self.ff1(combined)\n",
        "       out = self.sigmoid(out)\n",
        "       return out\n"
      ],
      "metadata": {
        "id": "eZZu9vOmt8tm"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "  epochs  = 3 #standard boilerplate\n",
        "  model = Classifier()\n",
        "  criterion = nn.BCELoss()\n",
        "  optimizer = optim.Adam(model.parameters())\n",
        "  \n",
        "  for epoch in range(epochs): #standard training procedure\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    tp_count = 0 #setup for f1 score\n",
        "    fp_count = 0\n",
        "    fn_count = 0\n",
        "    f1 = 0\n",
        "    \n",
        "    i = 0\n",
        "    for x,y, label in train_data:\n",
        "      optimizer.zero_grad()\n",
        "      x_embed = embed(x)\n",
        "      y_embed = embed(y)\n",
        "     \n",
        "      padding_length_x  = (MAX_TOKEN_DIM * 768 - x_embed.size()[0])\n",
        "      padding_length_y  = (MAX_TOKEN_DIM * 768 - y_embed.size()[0])\n",
        "      print( y_embed.size())\n",
        "      x_padded = torch.nn.functional.pad(x_embed, (int(padding_length_x/2), int(padding_length_x/2)))\n",
        "      y_padded = torch.nn.functional.pad(y_embed, (int(padding_length_y/2), int(padding_length_y/2)))\n",
        "     \n",
        "       \n",
        "      pred = model(x_padded,y_padded)\n",
        "\n",
        "      loss = criterion(pred,torch.tensor([label]))\n",
        "      loss.backward()\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "      #calculate scores\n",
        "      pred_rounded = torch.round(pred)\n",
        "      if pred_rounded == 1 and label == 1:\n",
        "        tp_count += 1\n",
        "      elif pred_rounded == 1 and label == 0:\n",
        "        fp_count += 1\n",
        "      elif pred_rounded == 0 and label == 1:\n",
        "        fn_count += 1\n",
        "      \n",
        "      if (tp_count + .5 * (fp_count + fn_count)) != 0:\n",
        "        f1 = tp_count/(tp_count + .5 * (fp_count + fn_count))\n",
        "      if i %1 == 0:\n",
        "        print(\"loss is {} and f1 is {}\".format(epoch_loss, f1))\n",
        "      i+=1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mDmyDuz6wuN7"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gsr8-zrrzR7_",
        "outputId": "e617c35b-2267-4c1d-b572-8d3497812465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([84480])\n",
            "loss is 0.6647986173629761 and f1 is 0\n",
            "torch.Size([153600])\n",
            "loss is 1.2669599652290344 and f1 is 1.0\n",
            "torch.Size([156672])\n",
            "loss is 1.8790569305419922 and f1 is 1.0\n",
            "torch.Size([195840])\n",
            "loss is 2.49358069896698 and f1 is 1.0\n",
            "torch.Size([330240])\n",
            "loss is 3.2037095427513123 and f1 is 0.8\n",
            "torch.Size([393216])\n",
            "loss is 3.8863039016723633 and f1 is 0.8\n",
            "torch.Size([208128])\n",
            "loss is 4.506139278411865 and f1 is 0.8\n",
            "torch.Size([228096])\n",
            "loss is 5.319403231143951 and f1 is 0.6666666666666666\n",
            "torch.Size([361728])\n",
            "loss is 6.089782655239105 and f1 is 0.5714285714285714\n",
            "torch.Size([393216])\n",
            "loss is 6.697390258312225 and f1 is 0.5714285714285714\n",
            "torch.Size([161280])\n",
            "loss is 7.378539264202118 and f1 is 0.6666666666666666\n",
            "torch.Size([393216])\n",
            "loss is 8.106647908687592 and f1 is 0.6\n",
            "torch.Size([297216])\n",
            "loss is 8.888766586780548 and f1 is 0.5454545454545454\n",
            "torch.Size([244224])\n",
            "loss is 9.555002868175507 and f1 is 0.5454545454545454\n",
            "torch.Size([393216])\n",
            "loss is 10.28296971321106 and f1 is 0.5\n",
            "torch.Size([393216])\n",
            "loss is 10.926194369792938 and f1 is 0.5714285714285714\n",
            "torch.Size([393216])\n",
            "loss is 11.622516572475433 and f1 is 0.5333333333333333\n",
            "torch.Size([393216])\n",
            "loss is 12.347638010978699 and f1 is 0.5\n",
            "torch.Size([179712])\n",
            "loss is 13.009284973144531 and f1 is 0.5555555555555556\n",
            "torch.Size([393216])\n",
            "loss is 13.658929109573364 and f1 is 0.5555555555555556\n",
            "torch.Size([393216])\n",
            "loss is 14.300462186336517 and f1 is 0.5555555555555556\n",
            "torch.Size([393216])\n",
            "loss is 14.908724427223206 and f1 is 0.5555555555555556\n",
            "torch.Size([393216])\n",
            "loss is 15.616565942764282 and f1 is 0.5263157894736842\n",
            "torch.Size([393216])\n",
            "loss is 16.406308591365814 and f1 is 0.5\n",
            "torch.Size([393216])\n",
            "loss is 17.135141134262085 and f1 is 0.47619047619047616\n",
            "torch.Size([393216])\n",
            "loss is 17.886877596378326 and f1 is 0.45454545454545453\n",
            "torch.Size([132096])\n",
            "loss is 18.59045773744583 and f1 is 0.43478260869565216\n",
            "torch.Size([362496])\n",
            "loss is 19.32586944103241 and f1 is 0.4166666666666667\n",
            "torch.Size([393216])\n",
            "loss is 20.124922692775726 and f1 is 0.4\n",
            "torch.Size([193536])\n",
            "loss is 20.77265340089798 and f1 is 0.4444444444444444\n",
            "torch.Size([393216])\n",
            "loss is 21.52637606859207 and f1 is 0.42857142857142855\n",
            "torch.Size([310272])\n",
            "loss is 22.292770564556122 and f1 is 0.41379310344827586\n",
            "torch.Size([393216])\n",
            "loss is 23.010858356952667 and f1 is 0.4\n",
            "torch.Size([111360])\n",
            "loss is 23.705308377742767 and f1 is 0.3870967741935484\n",
            "torch.Size([319488])\n",
            "loss is 24.476771593093872 and f1 is 0.375\n",
            "torch.Size([161280])\n",
            "loss is 25.1631920337677 and f1 is 0.4117647058823529\n",
            "torch.Size([393216])\n",
            "loss is 25.890173017978668 and f1 is 0.4\n",
            "torch.Size([237312])\n",
            "loss is 26.49709117412567 and f1 is 0.4\n",
            "torch.Size([168960])\n",
            "loss is 27.16436403989792 and f1 is 0.43243243243243246\n",
            "torch.Size([367104])\n",
            "loss is 27.833644151687622 and f1 is 0.43243243243243246\n",
            "torch.Size([310272])\n",
            "loss is 28.599814295768738 and f1 is 0.42105263157894735\n",
            "torch.Size([393216])\n",
            "loss is 29.32804536819458 and f1 is 0.41025641025641024\n",
            "torch.Size([219648])\n",
            "loss is 30.000003159046173 and f1 is 0.43902439024390244\n",
            "torch.Size([393216])\n",
            "loss is 30.673194885253906 and f1 is 0.46511627906976744\n",
            "torch.Size([278016])\n",
            "loss is 31.326463878154755 and f1 is 0.46511627906976744\n",
            "torch.Size([393216])\n",
            "loss is 31.962658524513245 and f1 is 0.46511627906976744\n",
            "torch.Size([352512])\n",
            "loss is 32.62181133031845 and f1 is 0.46511627906976744\n",
            "torch.Size([107520])\n",
            "loss is 33.308247804641724 and f1 is 0.46511627906976744\n",
            "torch.Size([393216])\n",
            "loss is 33.97565472126007 and f1 is 0.4888888888888889\n",
            "torch.Size([393216])\n",
            "loss is 34.69474768638611 and f1 is 0.4782608695652174\n",
            "torch.Size([393216])\n",
            "loss is 35.49469393491745 and f1 is 0.46808510638297873\n",
            "torch.Size([393216])\n",
            "loss is 36.099601209163666 and f1 is 0.46808510638297873\n",
            "torch.Size([393216])\n"
          ]
        }
      ]
    }
  ]
}